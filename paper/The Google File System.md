# The Google File System

原文链接：https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf

## 摘要

Google File System是针对大型数据密集型应用设计的可扩展分布式文件系统。GFS不仅可以运行在廉价的硬件上并提供容错保证，而且也可以为大量客户端提供高性能的服务。

## 简介

设计并实现GFS，是为了满足 Google 迅速增长的数据处理需求。GFS与传统的分布式文件系统有很多相同的设计目标，比如，性能、可伸缩性、可靠性以及可用性。但为了满足Google的应用负载情况和技术环境的影响，GFS将重新考虑传统分布式文件系统的设计这种选择。GFS将重点考虑以下几个方面：

1. **机器组件失败是常态**。

2. **以通常的标准相比，文件更巨大**。通常是数GB大小的文件，每个文件包含很多应用对象，比如web文档。相比于KB级别的小文件，更适合管理不断快速增长的TB级别的文件。

3. **文件修改以尾部追加为主，而不是覆盖**。对文件的随机写入操作在实际中几乎不存在。一旦写完之后，对文件的操作就只有读，而且通常是按顺序读。

## 设计概述

### 设计预期

+ 系统构建与大量廉价机器上，组件失效是常态。必须持续不断的对自身健康状况进行监控，容错，当组件失败时可以立即恢复。
+ 系统存储了很大数量的大文件，预期会有数百万计的大小在100M以上的文件。数GB大小的文件也很常见，需要有效的管理。支持小文件存储，但是不必优化。
+ 读造成的系统负载分为两种：大规模的流式读取和小规模的随机读。流式读取通常一次性读取数百KB或者1MB以上。一个客户端的连续操作通常是读文件的一个连续区域。随机读通常是在任意位置读几KB，为了提高性能，通常可以将小规模的随机读操作合并并排序，然后顺序批量读取，避免在文件中前后来回移动读取位置。
+ 写造成的系统负载主要是大规模的顺序追加方式写操作。写数据的量级和大规模读数据的量级差不多，数据一旦写入，几乎不会被修改。系统也支持小规模的随机写操作，但是不保证性能。
+ 需要高性能的实现语义明确的多端并发追加写文件操作。系统通常被用于生产者消费者队列或者多路合并的场景，通常会有上百个生产者同时向一个文件追加内容。使用最小的同步开销来实现的原子操作是必不可少的。可以在消费者追加的同时或者稍后读取文件。
+ 高稳定的网络带宽比低延迟重要。大多数应用场景都是高速的处理批量数据，极少情况会关心单次读写的响应时间。



### 接口

GFS提供了一套类似传统文件系统的API。文件以分层目录的形式组织，用路径名标识。支持的操作包括`create`, `delete`, `open`, `close`,`read`,`write`等。

另外，GFS还支持快照`snapshot` 和记录追加`record append`操作。

+ 快照：快速复制文件或者目录。
+ 记录追加：允许多端原子性的追加数据到同一个文件。

在生产者-消费者队列或者多路合并场景中，通过原子性的记录追加操作，可以避免使用额外的同步锁。



### 架构

GFS集群包括一个单独的主节点`master`和多个`chunckservers`，并可同时被多个`client`访问，如图1。

![avatar](../pic/GFS%20Architecture.png)

这些组件都运行在普通的Linux机器上，这些机器运行着各种用户级别应用。只要机器资源允许，可以很容易的将`chunk server`和客户端都放在一个机器上，并且可以承受其他不可靠应用程序造成的稳定性降低风险。

文件被分割成固定大小的`chunks`。当chunk被创建时，master会为每个chunk分配一个不可变的64位的全局唯一标示`chunk handle`。Chunkservers将chunks当作普通Linux文件存储在机器的本地磁盘上，并通过chunk handle和range来读写chunk。用户可以为不同的文件命名空间定义不同的chunk副本数量，默认为3副本。

**主节点master的作用**：

+ chunk lease management
+ 孤儿chunk的垃圾回收
+ chunk在server间的迁移

+ 管理元数据
+ 与chunkservers保持周期性的心跳，并收集其状态

用户程序通过调用GFS client API来访问master和chunkservers进行数据读写。客户端和master的通信只获取元数据，所有数据操作都是客户端和chunkservers直接进行的。

客户端会对元数据进行缓存，但不需要对文件数据进行缓存。因为大多数场景都是流式读取一个大文件或者数据集太大根本无法缓存。chunkservers也不需要对文件数据进行缓存，因为chunks是以本地文件的形式存储的，Linux本身会对经常访问的数据进行缓存。无需考虑缓存，简化了客户端和整个系统的设计和实现。



### 单主节点

单主节点可以利用全局信息进行chunk管理以及副本策略，极大的简化了设计。为了避免成为性能瓶颈，需要尽可能减少对单主节点的读写。一方面，客户端不通过主节点进行数据读写，另一方面，客户端会对元数据缓存一段时间，后续操作直接访问chunkservsers进行。

图1的数据读流程如下：

1. 根据chunk的大小，客户端将文件名和程序指定的字节偏移，转换成chunk索引
2. 将文件名和chunk索引发送给master
3. master将对应的chunk标示(chunk handle)和副本位置发送给客户端
4. 客户端使用文件名和chunk索引作为key，缓存这些元数据
5. 客户端发送请求给其中一个副本，通常会选最近的一个。请求包括了chunk标示和读取范围。

因为第一次请求时客户端缓存了元数据，之后再读同一个chunk的内容时，就不需要再访问master了，除非缓存过期或者文件被重新打开了。实际中，客户端会一次访问多个chunk，master也会将紧跟的后续chunk元数据一并返回。这样当未来顺序访问后续chunk时，也不需要有再次访问master的开销了。



### chunk大小

chunk大小是一个关键的设计参数，GFS选择64MB，这比普通的文件系统块大小要大得多。每个chunk副本都是以普通Linux文件存储在chunkservers上的，只在需要的时候才扩大。chunk选择64M最大的争议是可能导致过多的内部碎片，使用惰性空间分配可以一定程度避免这个问题。

选择一个比较大的chunk尺寸优点如下：

+ **减少client对master的频繁访问**。访问一次master获取元数据后，之后就可以对同一个chunk进行多次读写。顺序读写大文件时，可以显著的降低系统负载。对于TB级数据的随机读写，客户端也可以通过一次性缓存所有chunk元数据来降低系统负载。
+ **减少网络负载**。当chunk尺寸比较大时，客户端可能对一个chunk进行多次操作，这时保存较长时间的TCP连接可以减少网络负载。
+ **降低master的元数据数量。**这使得将所有元数据保存在内存中成为可能。

及时使用惰性空间分配策略，较大的chunk也有一些缺点：

+ 小文件包含很少的chunk，甚至只有一个chunk。当许多客户端同时访问这个文件时，存储这些chunk的chunkservers就会成为热点。

虽然实践中主要场景时大文件的顺序读写，但是某些场景下小文件热点问题还是会出现。比如在GFS上保存了一个单chunk的可执行文件，然后数百个客户端同时启动这个可执行文件。可以通过提高副本数量，或者错开执行时间来解决这个问题。但是对于这个问题的长久解决方案是，允许客户端可以从其他客户端读取数据。



### 元数据

主节点主要存储了三种元数据：

- 文件和chunk的命名空间
- 文件和其chunk的映射关系
- chunk所在的服务器位置信息

所有的元数据都保存在内存中。前两种元数据也会以操作日志的形式持久化在主节点磁盘上，并会在其他远程机器上保存副本。使用操作日志的方式使主节点的状态更新变得更加简单可靠，并且可以防止主节点宕机时的不一致风险。对于最后一种元数据，主节点并不会持久化存储chunk位置信息。主节点只是在启动时或者新的chunkserver加入时向各个chunkserver轮训它们存储的chunk信息。

#### 内存数据结构

由于元数据保存在内存中，master的操作都可以很迅速。因此master可以很高效的在后台周期性的扫描全部状态信息。周期性的状态扫描是为了做chunk的垃圾回收、chunkserver失败时重新复制副本、为了负载均衡迁移chunk以及做磁盘使用状况统计。

一个潜在的问题是，chunk的数量以及集群所能支持的数据量受主节点内存大小制约。在实践中，这不是一个严重的问题。master管理一个64M大小的chunk只需要不到64字节的元数据。由于大多数文件都包括多个chunk，所以大多数chunk都是满的，只有最后一个chunk可能不是满的。同样的，因为使用了前缀压缩算法，每个文件的命名空间数据通常也是小于64字节的。

即便是要支持更大的系统，也可以通过增加额外内存的方式解决。

#### chunk位置

master不会持久化chunk的位置信息，即哪个chunkserver保存了指定的chunk副本。master只是在启动时轮训各个chunkservers来获取这些信息。通过不断向chunkserver发送心跳信息来监控chunkserver的状态，因此master能够随时保证这些信息是最新的。

master不持久化位置信息的原因是，当遇到chunkserver加入或离开集群、改名、失败、重启等等情况时，保证master和chunkservers的信息一致是很复杂的。而一个拥有数百台服务器的集群，上述情况是经常会发生的。

另一个角度来讲，只有chunkserver才能最终确定一个chunk是否在它自己的磁盘上。没有必要费力在master上维护一个这些信息的全局视图，因为chunk 服务器的错误可能会导致 Chunk 自动消失(比如，硬盘损坏了或者无法访问了)，或者操作人员可能会重命名一个 chunk 服务器。

#### 操作日志

操作日志记录了关键的元数据变更历史。操作日志对GFS系统非常重要，不仅是因为它是元数据唯一的持久化记录，还因为它可以作为判断并发操作顺序的逻辑时间基准。文件和chunk以及对应的版本，在创建时会被分配一个永远唯一的逻辑时间戳。

由于操作日志非常重要，必须要可靠的存储，并且在元数据变更被持久化之前，对客户端不可见。否则，即使 chunk 本身没有出现任何问题，仍有可能丢失整个文件系统或客户端最近的操作。因此，GFS会将操作日志在多个远程机器上备份，并且只有在把日志记录写入本地及远程机器的磁盘后，才会响应客户端操作完成。master会批量处理多个日志记录，以减少写入磁盘和网络复制对系统的开销。

master通过重放操作日志来恢复文件系统状态。日志越大重放操作时间越久，为了缩短master启动时间，需要保持日志足够小。当操作日志大小超过阈值，master就会将状态写入检查点文件。失败恢复时，master只需要加载最新的检查点文件，并执行检查点文件之后发生的操作日志即可。检查点文件使用压缩B树的形式保存，可以直接映射到内存中，无需外部解析即可用于命名空间查找。这进一步加快了恢复速度并提高了可用性。

由于创建检查点文件需要花费一些时间，为了不影响后续变更操作的记录，master会把操作记录写到一个新的日志文件，并使用一个独立的线程创建检查点文件。对于具有几百万个文件的群集，可以在一分钟左右创建检查点文件。创建完成后，检查点文件会被写入本地和远程机器的磁盘。

失败恢复只需要最新的检查点文件和其后续的操作日志。旧的检查点文件和操作日志可以被删除，但是为了应对灾难性的故障，我们通常会多保存一些历史文件。生成检查点文件失败不会对正确性产生影响，因为失败恢复时会校验并跳过不完整的检查点文件。

### 一致性模型

GFS 支持一个宽松的一致性模型，这个模型能很好的支撑高度分布的应用，同时还保持了相对简单且容易实现的优点。

#### GFS的一致性保证

文件命名空间变更(比如创建时)是原子性的。只有master可以变更文件命名空间，master一方面通过对命名空间加锁来保证原子性和正确性，一方面也会全局有序的将这类操作记录在操作日志中。

数据修改后文件region的状态取决于操作的类型、执行成功与否，以及是否是并发修改。下表总结了各种操作后文件region的状态。

|        | 写           | 记录追加           |
| ------ | ------------ | ------------------ |
| 串行写 | 已定义       | 已定义但部分不一致 |
| 并发写 | 一致但未定义 | 已定义但部分不一致 |
| 失败   | 不一致       | 不一致             |

状态包括一致`consistent`和已定义`defined`：

+ **一致**：无论读哪个副本，所有客户端总能看到相同的数据
+ **已定义**：一致，并且客户端可以看到全部已写的变更

数据变更包括写`writes`和记录追加`record appends`:

+ **写**：将数据写在==用户程序指定==的偏移位置
+ **记录追加**：在顺序或者并发变更时，能够原子性的至少一次将数据追加到==GFS指定==的偏移位置（与常规追加不同，常规的追加仅仅是将数据写到客户端认为的当前文件尾部）

> 注：记录追加后，GFS指定的偏移位置会返回给客户端，这个偏移位置指的是包含追加内容的已定义region的开头位置。另外，GFS有可能在region中插入填充数据或者重复的记录，虽然这些数据相比用户数据会非常小，但是这导致了region可能是不一致的。

1. GFS将变更以同样的顺序应用到chunk的所有副本上
2. 当chunkserver宕机，其上存储的chunk副本会错过变更。使用chunk的版本号检测副本是否失效，失效的副本不会再进行任务变更操作，它的位置也不会返回给客户端。失效的副本会被垃圾回收系统尽快回收。

由于客户端缓存了chunk的位置，当缓存数据刷新之前，很有可能访问到失效的副本。这种情况通常发生在缓存过期前或者文件重新打开之前。文件重新打开后，文件的所有chunk信息，会重新载入缓存。由于大多数文件都是只追加更新，所以访问失效的副本一般会返回一个提前结束的chunk而不是错误(过期)的数据。当客户端尝试重连master时，它就会立刻得到最新的chunk 位置信息。

即使在变更发生很久之后，组件的失效也可能损坏数据。GFS通过定期握手发现失效的chunkserver，并使用checksumming算法校验数据是否损坏。一旦发现问题，会尽快使用有效的副本恢复数据。除非在GFS检测并恢复数据之前（通常是几分钟），所有副本都失效或丢失了，一个chunk才不可逆转的失效了。即便在这种极端情况下，chunk也只是不可用了，而不是损坏了。也就是说，客户端会收到明确的错误信息，而不是收到错误的数据。

#### 实践影响

GFS使用一些简单的技术手段来实现上述宽松的一致性模型，这些技术也被用于GFS的其他功能目标，比如：主要采用追加而不是覆盖、创建检查点、自验证的写入操作、自标识的记录。

(文章这里描述两种典型的应用场景，不再赘述)



## 系统交互

设计GFS系统的原则之一是尽可能的减少各种操作和主节点的交互。基于这个原则，接下来会描述在执行数据变更、原子追加以及快照时，客户端、主节点以及chunkserver的交互流程。

### 租约和变更顺序

> **租约**（lease）在英文中的含义是“租期”、“承诺”，在分布式中一般描述如下：

> + Lease 是由授权者授予的在一段时间内的承诺。
> + 授权者一旦发出 lease，则无论接受方是否收到，也无论后续接收方处于何种状态，只要 lease 不过期，授权者一定遵守承诺，按承诺的时间、内容执行。
> + 接收方在有效期内可以使用颁发者的承诺，只要 lease 过期，接收方放弃授权，不再继续执行，要重新申请Lease。
> + 可以通过版本号、时间周期，或者到某个固定时间点认为Lease证书失效
>
> 关于Lease最经典的解释来源于Lease的原始论文<<Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency>>：
>
>   **a lease is a contract that gives its holder specific rights over property for a limited period of time**
> 即Lease是一种带期限的契约，在此期限内拥有Lease的节点有权利操作一些预设好的对象。从更深 层次上来看，Lease就是一把带有超时机制的分布式锁，如果没有Lease，分布式环境中的锁可能会因为锁拥有者的失败而导致死锁，有了lease死锁会被控制在超时时间之内。

变更指通过写或者追加的方式改变chunk的内容或者元数据。每个变更操作都会作用于全部chunk副本。我们使用租约来保证副本之前变更顺序的一致性。主节点会为其中一个副本授权租约，这个副本被称作主副本。主副本会为chunk的所有变更建立一个顺序序列，所有从副本都会遵从这个变更顺序。因此，全局的变更顺序，首先由主节点选择租约授权的顺序决定，然后由主副本分配的序列号决定。

  使用租约机制是为了减少主节点的管理开销。租约初始的超时时间是60秒，但如果chunk正在被修改，主副本可以向master请求并得到允许延长租约时间的许可。延长租约的请求及批准延长的许可信息，通常是附加在master和chunkservers的心跳消息中传递的。master也有可能在租约到期之前吊销租约（比如master想要停止一个正在重命名文件上的变更操作）。如果master和主副本失去联系，当租约过期后，也可以安全的向其他副本授权新的租约。

图2通过7个步骤展示了写操作的控制流程：

![图2：写操作的控制流和数据流](/Users/chimes/IdeaProjects/notes/pic/Write Control and Data Flow.png)

1. 客户端向master询问持有当前租约的chunkserver(即主副本所在位置)以及其他副本的位置。如果当前没有任何一个副本持有租约，master会先为一个副本授权租约。
2. master将主副本和其他从副本的位置信息返回给客户端。客户端会缓存这些元数据用于未来可能发生的变更操作。后续只有当无法访问到主副本或者租约超时，客户端才需再次访问master。
3. 客户端以任意顺序，将数据推送到所有副本。每个chunkserver会将这些数据存储到一个内部的LRU缓存，直到这些数据被写入或者过期。由于数据流的网络传输负载非常高，通过分离数据流和控制流，可以基于网络拓扑情况对数据流进行规划，提高系统性能，而不用去理会哪个chunkserver保存了主副本。
4. 一旦所有副本都完成数据接收，客户端会向主副本发送写请求。请求标示出了之前发送给所有副本的数据。然后主副本会为它收到的所有变更操作按照顺序分配序号，这些变更操作可能来自多个客户端。最后主副本会按照序号顺序执行变更，并更新自己的本地状态。
5. 主副本将写请求转发给所有从副本。每个从副本按照相同的序号顺序执行变更。
6. 所有从副本完成操作后，告知主副本
7. 主副本响应客户端完成写操作。如果在写任一副本的过程中遇到错误，都会报告给客户端。当遇到错误时，可能在主副本和任意从副本上已经写成功了(如果写主副本时失败，主副本将不会分配序号并转发写请求)。这时，客户端的请求会收到写失败的响应，同时被修改区域也会留下数据不一致的状态。客户端程序通常通过重拾失败的变更来解决这个问题，在从头开始重拾之前，会先将3-7这几个步骤重试几次。

如果一次写入的数据量很大，或者跨越了多个chunk，客户端会将操作拆分成多个写操作。这些写操作都会按照上述步骤执行，但是在多端并发操作的情况下，这些操作可能和其他客户端的操作交错并被覆盖。所以文件region可能包含来自不同客户端的数据片段。虽然每个副本按照相同顺序执行每个独立的操作可以保证文件region的数据一致性，但是状态可能是未提交的(因为并发写操作可能覆盖数据，客户端可能无法看到全部的已写变更)。

### 数据流

为了提高网络性能，将数据流和控制流解耦。控制流从客户端发送到主副本再到次副本的同时，数据流以管道的形式，沿着精心选择的chunkservers链线性推送。目标是充分利用服务器间的网络带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时。

为了充分利用网络带宽，数据沿着chunkservers链线性推送，而不是以树等其他拓扑形式分发。这样做，每台机器可以用全部带宽以尽可能快的速度传输数据，而不用在多个接收者之间分配带宽。

为了尽可能避免网络瓶颈和高延迟链接，每台服务器都在网络拓扑中选一台离自己"最近的"还没接收到数据的机器推送。网络拓扑足够简单，通过IP地址就可以准确的估计“距离”。

通过建立TCP链接，以管道的形式传输数据，以此来降低延迟。一旦chunkserver收到数据，立刻转发。因为采用全双工的网络交换，所以管道的方式推送数据有很大好处，这样接收到数据立刻转发也不会降低接收的速度。

### 原子记录追加

GFS提供原子性追加操作，称为`record append`。传统的写操作，客户端会决定写入的位置(偏移量)，并发写对同一区域的写操作不是串行的，文件写入的区域可能会包括来自多个端的数据片段。而对于记录追加的操作，客户端只能指定写什么数据，GFS会自动将这些数据至少一次追加到文件尾部。追加的这个尾部位置，是GFS决定的，并最终会把这个位置返回给客户端。这类似于在 Unix 操作系统编程环境中，对以 O_APPEND 模式打开的文件，多个并发写操作在没有竞争条件时的行为。

记录追加的操作在分布应用中使用非常频繁，通常会有多个客户端并发地对同一个文件追加写入数据。如果我们采用传统方式的文件写入操作，客户端需要额外的复杂且高代价的的同步机制，例如使用一个分布式的锁管理器。

记录追加也是一种变更操作，和之前所述的流程差不多，只对于主副本有一些额外的逻辑。当客户端将数据发送给所有副本后，它会给主副本发送写请求。这时主副本会检查所要追加的记录大小是否超过了一个chunk的默认大小64M。如果超过的话，主副本会先将当前的chunk填充满，并告知从副本也如此操作，然后答复客户端，追加操作需要在下个chunk上继续(记录追加的数据大小严格控制在 Chunk 最大尺寸的 1/4，这样即使在最坏情况下，数据碎片的数量仍然在可控的范围)。如果没有超过最大尺寸，追加流程就和正常情况一样。

如果在某个副本上追加操作失败，客户端会重试这次操作。重试追加操作的结果是，同一个chunk的不同副本，可能重复的包含一条记录的全部或者部分。GFS不保证所有副本在字节级别是完全一致的，只能保证数据是原子性的被至少一次写入。这个特性可以通过简单观察推断出来：如果操作成功，数据一定被写到这些chunk的所有副本的相同偏移位置上。因此，所有副本记录的长度至少和记录是一样的(有可能重复，所以只多不少)。在这之后，即使有其他副本变成新的主副本，新的记录也会在更高的偏移位置或者新的chunk上记录。就之前提到的一致性模型而言，对于所有成功追加记录的区域，数据是已定义的(也就是一致的)，而对于可能重复的区域，数据是不一致的(也就是未定义的)。

### 快照

快照操作能够在瞬间完成文件或目录的复制，并几乎不会影响正在发生的变更操作。用户可以通过快照快速创建巨大数据集的分支备份(通常是对备份进行递归备份)。或者是在做一些实验性的操作之前，将当前状态计入检查点，这样之后提交或者回滚都会很简单。

如AFS(Andrew File System，一种分布式文件系统)一样，通过标准的copy-on-write(先复制，在复制的副本上写)技术实现快照。当master收到快照请求，它首先会取消快照文件chunk上的所有未完成租约。这一步是为了确保，所有后续的写操作都必须和master交互来找到最新的租约持有副本。这就给master一个先去创建chunk备份的机会。

在所有租约都到期或者取消后，master先将快照操作记入操作日志。然后通过复制文件或目录在内存中元数据的方式来完成快照操作。新产生的快照文件指向源文件相同的chunk。(在这一步其实只复制了元数据，并没有对实际数据复制)

当快照操作后第一次客户端请求对chunk C进行写操作时，会先向master查询当前持有租约的副本。这时，master会发现chunk C的引用计数器大于1了(因为快照复制的元数据也指向这个chunk)，然后master会返回客户端一个新的句柄C‘，并通知副本所在的chunkserver创建一个新的chunk C'。通过在原始chunk所在的机器上创建新chunk，可以确保所有数据复制都是本地复制，没有网络传输(此时才真正复制数据)。在这之后，请处理C' 和请求对任务其他chunk没有区别：master会和C'的其中一个副本授权租约，并响应客户端，然后客户端就可以正常的写数据，并且不必知道它写的C‘是刚从已存在chunk复制来的。

## 主节点操作

master负责所有命名空间相关的操作。另外，master还管理整个文件系统chunk和其副本，比如决定副本所在的位置，创建chunk并生成副本，协调各种系统级别活动以保证chunk充分备份，在所有chunkserver间实现负载均衡以及回收不再使用的存储空间。

### 命名空间管理及加锁

master的许多操作会花费比较长的时间，比如快照操作时，取消快照涉及chunk所在server的租约。为了使某个操作不延迟其他操作执行，master允许多个操作并行执行，并通过在操作相关的region上加锁，来保证共享区域操作执行的顺序。

不同于许多传统文件系统，GFS 没有针对每个目录实现能够列出目录下所有文件的数据结构， 也不支持文件或者目录的软硬链接。命名空间可以看作一个从全路径名到元数据的查找表。利用前缀压缩技术，这个表可以高效的存储在内存中。命名空间的树形结构上，每一个节点(绝对文件名或者目录名)都有一个关联的读写锁。

master的每个操作运行前，都会获取到一系列锁。通常，一个作用与路径 /d1/d2/.../dn/leaf 的操作，首先会获取到/d1, /d1/d2, …, /d1/d2/.../dn这些目录的读锁，并且获取全路径/d1/d2/.../dn/leaf的读锁或者写锁。其中leaf有可能是文件也可能是目录，看是什么操作了。

接下来描述一下锁的机制，当路径/home/user通过快照操作复制到/save/user时，如何防止创建文件/home/user/foo。快照操作会获取路径/home和/save的读锁，以及路径/home/user和/save/user的写锁。创建文件操作会获取路径/home和/home/user的读锁，以及路径/home/user/foo的写锁。快照和创建文件两个操作需要顺序执行，因为它们在尝试获取路径/home/user的锁时会产生冲突。文件创建操作不需要在父路径加写锁，因为并没有实际的“目录”，或者类似inode这样的数据结构可以防止修改。加读锁就足够防止父路径被删除了。

采用这种锁方案的好处是，系统可以对同一目录进行并发变更操作。比如，可以在同一路径下同时创建多个文件：每次创建操作都获取一个目录的读锁和文件的写锁。对目录的读锁用于防止其他操作对目录删除、重命名或者快照。文件的写锁，会让对文件的操作顺序执行，防止多次创建同名文件。

由于命名空间可能包含多个节点，所以读写锁对象会被惰性分配，并且在使用后立刻删除。锁的获取需要依据全局一致的顺序来避免死锁：首先按命名空间的层次排序，在同一个层次内按字典顺序排序。

### 副本位置

GFS集群副本高度分布于多个层级而不是一个水平层级。通常一个集群会包括上百个chunkservers分布在多个机架上。这些chunkservers又可能被来自同一个或者不同机架的数百个客户端访问。不同机架的两台机器通信，通常需要跨域不止一个网络交换机。另外，进出机架的带宽可能小于机架内所有机器的总带宽。多层级分布的集群环境，对可扩展性，可靠性和可用性都提出了独特的挑战。

chunk副本位置选择遵循两个目标：

1. 最大化数据可靠性和可用性，最大化网络带宽利用率
2. 将副本分发到不同的机架，而不仅是多个机器

这样可以防止整个机架故障(比如交换机或者电源故障)导致的数据不可用。这样做还可以有效率用多个机架的带宽，特别是对于读操作。但是另一方面，写操作不得不进行多个机架的网络通信，但是这个代价是值得的，因为大部分场景是写一次，读多次。

### chunk的创建、重新创建副本以及负载均衡

chunk的副本在三种情况下被创建：chunk创建、重新产生副本、负载均衡。

当master创建chunk的时候，会基于以下三个原则考虑在哪里初始化空副本：

1. 将新的副本放在磁盘利用率低于平均水平的机器。
2. 尽可能不将临近的操作分配在在同一个chunkserver上。及时创建chunk本身没有什么开销，但是创建文件就意味着马上会有繁重的写操作。对于追加一次读多次这种工作模式，一旦数据写操作结束，文件就相当于只读的了。
3. 如上一节讨论，尽可能将chunk的副本分布到不同的机架上。

当可用的副本数量小于用户指定的阈值时，master会立刻重新创建副本。这种情况发生在：chunkserver不可用、chunkserver上报副本已损坏、磁盘损坏或者要求的副本数量增加。

每个需要重新创建副本的chunk，也会有优先级。决定优先级的一个指标是，当前副本数与期望的副本数的差距，对于已经丢失两个副本的chunk，比只丢失一个副本的chunk优先级更高。另外，相比于将要被删除的chunk，系统会优先给活跃文件的chunk重新创建副本。最后，为了避免影响正在运行的程序，系统会给正在被客户端访问的chunk更高的优先级。

master会选择一个优先级最高的chunk，指定一些chunkservers对已存在的有效副本进行复制。选择新副本所在位置的原则，和上文创建chunk时选择位置的原则一样。为了避免重新创建副本的流量负载影响客户端操作，master会限制集群内和单服务器上同时进行复制操作的数量。另外，每个chunkserver也会限制其对源副本所在chunkserver的读请求来限制带宽。

master会周期性的对副本做负载均衡操作：它会检查当前副本的分布状况，然后移动副本来获取更好的磁盘空间和负载均衡。通过负载均衡操作，master会逐渐填充满一个chunkserver，而不是立刻创建新chunk用繁重的写操作淹没服务器资源。负载均衡放置副本的策略也和之前提到的基本一样，不同的是，master还需要考虑需要删除哪些副本。通常，master会选择磁盘空间低于标准的chunkserver删除。

### 垃圾回收

文件被删除后，GFS并不会立即回收其占用的物理空间，而是通过惰性的针对chunk或者文件执行垃圾回收操作。这种方式使系统更简单可靠。

#### 机制

当文件被应用程序删除后，master会将这个操作记入操作日志。master不会立刻释放文件资源，而是先重命名为一个包含删除时间戳的隐藏文件。当master例行检查文件系统命名空间时，删掉所有已经超过三天(可配置)的隐藏文件。在此之前，都可以通过重命名后的特殊名字访问到文件，并且可以通过重命名为正常名字恢复文件。当隐藏文件都从命名空间删除后，内存中的元数据也会擦除，这就切断了其所有块的链接。

在例行检查chunk命名空间时，master也会检查孤儿chunk(任何文件都不可达的chunk)，然后擦除这些chunk的元数据。在master和chunkserver的定期心跳信息中，每个chunkserver会上报自己存储的chunk集合，然后master回复元数据中已经不存在的chunk标示，chunkserver收到后，就可以自由的删掉这些chunk的副本。

#### 讨论

虽然分布式垃圾回收在编程语言领域是一个需要复杂的方案才能解决的难题，但是在 GFS 系统中是非常简单的。我们可以轻易的得到 chunk 的所有引用：文件到chunk的映射关系都保存在master的内存中。我们也可以轻易的获得chunk的所有副本：它们以Linux文件的形式存储在chunkserver指定目录下。所有master不能识别的副本都被当作垃圾。

相比立即挥手立即删除，垃圾回收的方法有一系列好处：

+ 在一个组件失效是常态的大规模分布式系统中，垃圾回收的方式简单又可靠，垃圾收集提供了一种统一且可靠的方法来清理任何未知的副本。
  + chunk创建时，可能在一些chunkserver上成功创建了副本，另一些没有。master可能不知道这些已创建的副本存在。
  + master和chunserver之间的副本检测的消息可能丢失，这时master不得不去重发。
+ 把资源回收的工作交给master后台处理，比如定时扫描命名空间和与chunkserver握手。
  + 把性能成本分摊到各个批次中。
  + 当master比较空闲时操作，避免影响响应客户端
+ 延迟回收资源可以防止意外和误操作，为不可逆删除提供容错余地

从经验来看，这种方式主要的缺点是，当资源紧张时，延迟删除可能让用户想要微调存储空间变得比较麻烦。重复创建和删除临时文件时，释放的存储空间不能马上重用。可以通过加快存储回收来解决这些问题。对于命名空间的不同部分，用户可以使用不同的副本策略和资源回收策略。比如，用户可以指定某目录树下文件的所有chunk都不另外复制副本，并且任何已删除的文件都会立即且不可撤销地从文件系统状态中删除。

### 过期副本检测

如果chunkserver失败，其上存储的chunk的副本可能变为失效状态并丢失后续所有的变更操作。对于每个chunk，master使用一个chunk版本号来区分正常的副本和失效的副本。

当master授权一个新的租约给chunk时，它会递增chunk版本号并通知副本更新。master和副本都会将这个新版本记录到他们的持久化状态中。这个过程发生在master响应客户端之前，在记录新的版本号之后，客户端才会开始写操作。如果副本不可用，chunk版本号不会增加。当chunkserver重启时，会向master汇报其上存储的chunk集合以及对应的版本号，这是master就会检测到chunkserver上的失效副本。如果 Master 节点看到一个比它记录的版本号更高的版本号，master会认为它和 chunkserver签订租约的操作失败了，因此master会重新选择一个更大的版本号以保证数据最新。

master会在定期的垃圾回收中删掉失效副本。在删掉失效副本之前，master会当作失效副本不存在，当客户端请求chunk的副本信息时，不会返回失效副本的信息给客户端。另一方面，当chunkserver持有一个chunk的租约或者复制操作时需要从另一个chunkserver读chunk时，master响应客户端的信息中也会包括版本号。在执行操作前，客户端和chunkserver都会对版本号进行验证，以保证不会操作过期的数据。

## 容错和诊断

设计GFS系统的一大挑战就是要面对高频的组件失败。组件的数量和质量都使得一些异常情况变得很常见：我们既不能完全信任机器，也不能完全信任磁盘。组件故障可能导致数据不可用甚至数据损坏。这一节讨论如何应对这些不可避免的挑战以及如何建立有效的工具来诊断这些故障。

### 高可用

考虑到由上百台服务器组成的GFS集群中，随时可能有一些不可用，我们通过两个简单的策略保证整个系统的高可用：快速恢复和备份。

#### 快速恢复

master和chunkserver都被设计成能够在数秒内重启并恢复状态，无论它们之前是因何原因停止的。事实上，不需要区分服务器到底是正常停止还是异常情况停止。通常，服务可能是因为被直接 kill 掉进程而停止。这种情况从客户端的角度来看，可以算是比较小的系统抖动，比如正在发出的请求可能会超时，然后通过重试重新连接到重启的服务器。

#### chunk备份

如前所述，每个chunk都会有多个副本，分布在多个机架的多个服务器上。对于不同的命名空间，客户端可以指定不同的备份等级，默认是3。当检测到chunkserver离线或者通过checksum方式发现副本数据损坏，为了保证有效备份数量，master会复制新的副本。虽然备份策略已经可以很好的满足高可用需求，但是面对日益增长的只读存储需求，我们也在探索其他的跨服务器冗余解决方案，比如纠删码。在我们高度解耦的系统中，实现这些高复杂性的冗余方案很有挑战性，但因为我们主要的工作负载是追加写和读而不是随机写，所以这些方案并非不可实现。

#### master备份

master的状态也会被可靠的备份，它的操作日志和检查点会在多个机器上备份。只有当变更操作的操作记录已经被存储到本地磁盘和远程机器之后，才会在master的状态中确认这次操作。master会处理所有来自客户端的变更操作以及垃圾回收之后的后台操作。当master遇到故障，会立即重启机器。如果是机器或者磁盘的故障，导致无法重启，独立于GFS的外部监控会启动一个新的master，并通过其他机器上备份的操作记录备份恢复。客户端通过别名访问master(比如gfs-test)，这个别名类似于DNS别名，当master被迁移到其他机器时，可以通过改变别名访问。

系统中还包含一些影子mater，在主master宕机时，对外提供文件系统的只读功能。它们是影子master，而非镜像，相比于主master通常会有不到1秒的轻微延迟。因此，对于不经常变更的文件或者客户端程序对于轻微延迟不敏感时，影子master能够提高读的效率。实际上，由于数据内容是从chunkserver上读的，客户端程序是不会读到过期数据的。在这个短暂的时间窗内，过期的只是文件的元数据，比如目录的内容或者访问控制信息。

为了保证数据一致性，影子master会读取不断增长的操作记录日志副本，并以与主master相同的顺序执行变更。与主master相同，影子master启动后会轮询chunkserver，以获取每个chunk副本的位置，之后会定期握手以监控chunkserver的状态。影子master只依赖于主master获取副本位置的更新信息，比如当主节点决定创建或者删除副本时。

### 数据完整性

chunkserver使用checksumming方法检测数据是否损坏。一个GFS集群通常在上百台机器上占有上千块硬盘。磁盘损坏导致数据在读写过程中损坏或者丢失是非常常见的。可以通过其他的副本来恢复损坏的数据，但是跨chunkserver比较副本来检查数据是否损坏在工程上很不切实际。另外，副本之前有些分歧可能是合法的：根据GFS变更操作的语义，如前文讨论的原子记录追加操作，并不能保证数据完全一致。每个chunkserver必须独立维护 checksum 来校验自己的副本的完整性。

将每个chunk分成64KB大小的块，每个块对应一个32位的checksum。与其他元数据一样，checksums存储在内存，同时会通过操作日志持久化到磁盘，并且和文件数据是隔离开的。

当数据读取时，再响应客户端或者其他chunkserver请求并返回数据之前，会先校验读取数据范围内对应的数据块checksum。因此，chunkserver不会将错误的数据返回给其他机器。如果数据块与记录的checksum不一致，chunkserver会向请求者返回错误，并向master上报错误。然后，请求者就会向其他副本请求数据，而master会从其他副本复制一个新的副本。当新的副本就绪后，master会通知副本错误的chunkserver删掉错误的副本。

校验checksum对读性能影响很小，原因如夏：

+ 大部分读操作都会至少读几个块，因此数据校验只会涉及到很小一部分额外数据。
+ GFS 客户端代码通过每次把读取操作都对齐在 checksum 块的边界上，进一步减少了这些额外的读取操作的负面影响。
+ checksum维护在内存中，查找和比较都不需要额外的IO操作，而checksum的计算可以和IO操作同时进行

checksum 的计算针对在 Chunk 尾部的追加写入操作做了高度优化（与之对应的是覆盖现有数据的写入操作），因为这类操作在我们的工作中占了很大比例。只增量更新最后一个不完整的块的 checksum，并且为追加填充的新块计算新的checksum。对于最后一个未完成的块，增量更新这个块的checksum，对于其他所有追加填充满的新的块，计算新的checksum。即使最后一个未填充的块出现数据损坏且无法立刻检测到，增量更新的新checksum值也会和已经存储的数据不一致，这样在下一次读操作时，就会检测到数据损坏。

相比之下，如果写操作覆盖了chunk的一个已存在的范围，我们必须读取和校验被覆盖范围所在的第一个和最后一个块，然后再执行写操作，操作完成之后再重新计算和写入新的 checksum。如果不校验第一个和最后一个被写的块，那么新的 checksum 可能会隐藏没有被覆盖区域内的数据错误。（覆盖的范围，收尾并不一样完全包含一个完整的块，假如第一个块或者最后一个块，未被覆盖的部分有数据损坏，不校验直接写，之后就会对错误数据生成checksum）

在 chunkserver空闲的时候，会扫描和校验不活动的chunk内容，这样做使得很少被读到的chunk数据完整性也能保证。一旦发现有chunk数据损坏，master可以创建一个新的、正确的副本，然后把损坏的副本删除掉。这个机制也避免了非活动的、已损坏的chunk欺骗master节点，使 Master 节点认为它们已经有了足够多的副本了。

### 诊断工具











